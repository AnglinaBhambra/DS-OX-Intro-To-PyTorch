{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('ostalk': conda)",
   "display_name": "Python 3.8.5 64-bit ('ostalk': conda)",
   "metadata": {
    "interpreter": {
     "hash": "17d3f4ce4447a268feaa606d9b06ecb649ec6540bb255cf7e90113e860e0b7eb"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Part A - Linear Regression from scratch with PyTorch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "After showing what the data looks like (from Notion), refer to the article - so people can see there example and how it looks visibally (also emphasise how important their blog posts and articles are)!\n",
    "\n",
    "Also mention above the heading Part A & Part B\n",
    "\n",
    "Ask Arvindra if I should cut out Part A (for the the presentation at least)\n",
    "\n",
    "Disclaimer about the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "source": [
    "Our model will predict the number of sales for 2 products (target variables) by looking at the number of Sales People, the number of Social Media Posts, as well as the amount of money spent on Google Ads (input variables or features) per month. Here’s the training data:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<img src=\"LinearRegressionData.png\" style=\"width:800px;height:200px\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This walk-through is inspired by the team at [Jovian.ml](jovian.ml) [[1](https://medium.com/@aakashns/linear-regression-with-pytorch-3dde91d60b50)]. Jovian.ml was a course that I followed that helped me immensely, and I also liked how there were multiple target variables."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "`sold_product1  = w11 * people + w12 * ads + w13 * posts + b1` \n",
    "\n",
    "`sold_product2 = w21 * people + w22 * ads + w23 * posts + b2`\n",
    "\n",
    "Each target variable is calculated to be a weighted sum of the input variables, offset by some constant, known as a bias.\n",
    "\n",
    "We use optimisation techniques to adjust the weights to make better predictions. The most common technique for Linear Regression is known as Gradient Descent."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input (people, ads, posts)\n",
    "inputs = np.array([[8, 500, 6], \n",
    "                   [5, 200, 15], \n",
    "                   [6, 100, 12], \n",
    "                   [4, 300, 8], \n",
    "                   [6, 250, 3]], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targets (product1, product2)\n",
    "targets = np.array([[40, 20], \n",
    "                    [25, 15], \n",
    "                    [15, 10], \n",
    "                    [20, 15], \n",
    "                    [25, 20]], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[  8., 500.,   6.],\n        [  5., 200.,  15.],\n        [  6., 100.,  12.],\n        [  4., 300.,   8.],\n        [  6., 250.,   3.]])\ntensor([[40., 20.],\n        [25., 15.],\n        [15., 10.],\n        [20., 15.],\n        [25., 20.]])\n"
     ]
    }
   ],
   "source": [
    "# Convert inputs and targets to tensors\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "source": [
    "We first start by randomely initialising the weights and biases matrices. We will see that the first row (/element) will be used to predict the first target variable."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-1.0462,  1.0804,  0.2616],\n        [ 0.4201, -0.8799,  0.2832]], requires_grad=True)\ntensor([0.3321, 2.2773], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Randomely initialise the weights and biases matrices\n",
    "w = torch.randn(2, 3, requires_grad=True)\n",
    "b = torch.randn(2, requires_grad=True)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "source": [
    "Here we have used `torch.randn` to create 2 tensors, `w` and `b`, with dimensions (2 x 3) and  (2 x 1) filled with random elements.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    return x @ w.t() + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 533.7485, -432.6068],\n        [ 215.1112, -167.3523],\n        [ 105.2371,  -79.7928],\n        [ 322.3698, -257.7433],\n        [ 264.9480, -214.3244]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculate pred\n",
    "preds = model(inputs)\n",
    "print(preds)"
   ]
  },
  {
   "source": [
    "Seeing as we randomely initialised our weights and biases matrices, which were used to make our predictions, we can see that our predictions are nowhere near out targets. So now we will use some sort of __loss function__ to evaluate our model's performance, then adjust our weights and biases matrices."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE loss\n",
    "def mse(t1, t2):\n",
    "    diff = t1 - t2\n",
    "    return torch.sum(diff * diff) / diff.numel()"
   ]
  },
  {
   "source": [
    "This function calculates the Mean Squared Error (MSE) (which is our loss function). The MSE calculates the difference between our 2 tensors (preds and targets), then squares each element of this matrix to remove the negative values. The average of the elements is then calculated, then returned as a single number.\n",
    "\n",
    "`torch.sum` - sums every element in a tensor.\n",
    "\n",
    "`torch.numel()` - calculates the number of elements in a tensor."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(81254., grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compute loss\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "source": [
    "We can now see how far off each prediction is from each target, by taking the square root of this metric.\n",
    "\n",
    "The targets themselves range from 1-40, so this loss is very high.\n",
    "\n",
    "Hence we know that the model is performing the best when the loss is at it's lowest."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-1.0462,  1.0804,  0.2616],\n        [ 0.4201, -0.8799,  0.2832]], requires_grad=True)\ntensor([[  1618.2268,  88923.6328,   2007.1614],\n        [ -1513.6586, -82431.4375,  -1882.6719]])\n"
     ]
    }
   ],
   "source": [
    "# Gradients for weights (the derivative of the loss with respect to the weights, d(loss)/dw)\n",
    "print(w)\n",
    "print(w.grad)"
   ]
  },
  {
   "source": [
    "Calculating `d(loss)/dw` (the gradients for the the weights) is useful, as we are able to find the set of weights where the loss is the lowest. This is possible as the loss is just a quadratic function of our weights and biases.\n",
    "\n",
    "If `d(loss)/dw` is positive:\n",
    "- increasing the element’s value slightly = increases the loss.\n",
    "- decreasing the element’s value slightly = decreases the loss.\n",
    "\n",
    "When the gradient element is negative:\n",
    "- increasing the element’s value slightly = decreases the loss.\n",
    "- decreasing the element’s value slightly = increases the loss.\n",
    "\n",
    "The change in loss by changing a weight element is __proportional__ to the value of the gradient of the loss w.r.t. that element.\n",
    "\n",
    "__We adjust the weights by subtracting a small quantity proportional to the gradient.__\n",
    "\n",
    "The gradients of the weights and biases then have to be reset to 0, because PyTorch accumulates the gradients. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust weights & reset gradients\n",
    "with torch.no_grad():\n",
    "    w -= w.grad * 1e-5\n",
    "    b -= b.grad * 1e-5\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "source": [
    "We use `torch.no_grad()` to tell PyTorch that we don't want to track these changes when updating the weights and biases, as PyTorch accumulates. Then we reset the gradients to 0.\n",
    "\n",
    "The __learning rate__ is an important hyperparameter that ensures that the gradients aren't drastically changed. It is typically a small number that we multiply the gradients with. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-1.0624,  0.1912,  0.2415],\n        [ 0.4353, -0.0556,  0.3020]], requires_grad=True)\ntensor([0.3295, 2.2798], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(747.4018, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculate loss\n",
    "preds = model(inputs)\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "source": [
    "We have only used gradient descent once, and can already see that the loss has decreased a lot!\n",
    "\n",
    "We can continue to minimise the loss by iterating upon this process, where each iteration is called an __epoch__."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for 100 epochs\n",
    "for i in range(100):\n",
    "    preds = model(inputs)\n",
    "    loss = mse(preds, targets)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * 1e-5\n",
    "        b -= b.grad * 1e-5\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(25.4727, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[42.4408, 22.7007],\n",
       "        [18.6433, 15.0181],\n",
       "        [ 7.0086, 11.5332],\n",
       "        [27.6112, 15.5023],\n",
       "        [19.2839, 13.3669]], grad_fn=<AddBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[40., 20.],\n",
       "        [25., 15.],\n",
       "        [15., 10.],\n",
       "        [20., 15.],\n",
       "        [25., 20.]])"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "source": [
    "## Part B - Using PyTorch built-ins for Linear Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# torch.nn contains utility classes for building neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input (people, ads, posts)\n",
    "inputs = np.array([[8, 500, 6], [5, 200, 15], [6, 100, 12],\n",
    "                   [4, 300, 8], [6, 250, 3], [6, 300, 8], \n",
    "                   [7, 250, 20], [8, 350, 15], [8, 150, 8], \n",
    "                   [6, 150, 24], [5, 400, 20], [7, 350, 22], \n",
    "                   [9, 300, 16], [8, 50, 18], [8, 150, 20]], \n",
    "                  dtype='float32')\n",
    "\n",
    "# Targets (product1, product2)\n",
    "targets = np.array([[40, 20], [25, 15], [15, 10], \n",
    "                    [20, 15], [25, 20], [35, 25], \n",
    "                    [30, 25], [35, 25], [20, 12], \n",
    "                    [15, 10], [35, 10], [30, 17], \n",
    "                    [30, 26], [10, 16], [20, 14]], \n",
    "                   dtype='float32')\n",
    "\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)"
   ]
  },
  {
   "source": [
    "As you can see, we are using a larger dataset (15 training examples) which we can split into smaller batches.\n",
    "\n",
    "`TensorDataset` - a list of tuples where each tuple corresponds to one point (input, target) (otherwise known as (feature, label)), and provides standard APIs for working with many different types of datasets in PyTorch. [[2](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset)]\n",
    "\n",
    "`DataLoader` - useful when working with large datasets as you can split the data into batches of a predefined size while training. It also provides other utilities like shuffling and random sampling of the data.\n",
    "\n",
    "We can reduce the loss more efficiently using the `shuffle` flag, which shuffles the training data before creating batches, to randomise the input to the optimisation algorithm."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[  8., 500.,   6.],\n",
       "         [  5., 200.,  15.],\n",
       "         [  6., 100.,  12.]]),\n",
       " tensor([[40., 20.],\n",
       "         [25., 15.],\n",
       "         [15., 10.]]))"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "# Define dataset\n",
    "train_ds = TensorDataset(inputs, targets)\n",
    "train_ds[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DataLoader\n",
    "batch_size = 5\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[  4., 300.,   8.],\n        [  7., 350.,  22.],\n        [  8., 350.,  15.],\n        [  6., 100.,  12.],\n        [  5., 200.,  15.]])\ntensor([[20., 15.],\n        [30., 17.],\n        [35., 25.],\n        [15., 10.],\n        [25., 15.]])\n"
     ]
    }
   ],
   "source": [
    "# How we typically use the DataLoader\n",
    "for xb, yb in train_dl:\n",
    "    print(xb)\n",
    "    print(yb)\n",
    "    break"
   ]
  },
  {
   "source": [
    "In Part A, we initialised the weights and biases manually, but we can do this more efficiently with PyTorch's `nn.Linear` class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Parameter containing:\ntensor([[ 0.5690, -0.0496, -0.0284],\n        [-0.5325, -0.0467,  0.1187]], requires_grad=True)\nParameter containing:\ntensor([0.2707, 0.3805], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = nn.Linear(3, 2)\n",
    "print(model.weight)\n",
    "print(model.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.5690, -0.0496, -0.0284],\n",
       "         [-0.5325, -0.0467,  0.1187]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.2707, 0.3805], requires_grad=True)]"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "# Parameters containing all of the weights and biases\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-20.1504, -26.5200],\n",
       "        [ -7.2311,  -9.8430],\n",
       "        [ -1.6166,  -6.0609],\n",
       "        [-12.5620, -14.8117],\n",
       "        [ -8.8020, -14.1347],\n",
       "        [-11.4241, -15.8766],\n",
       "        [ -8.7154, -12.6499],\n",
       "        [-12.9650, -18.4462],\n",
       "        [ -2.8454,  -9.9357],\n",
       "        [ -4.4373,  -6.9722],\n",
       "        [-17.2940, -18.5908],\n",
       "        [-13.7326, -17.0831],\n",
       "        [ -9.9442, -16.5247],\n",
       "        [  1.8313,  -4.0785],\n",
       "        [ -3.1859,  -8.5118]], grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "source": [
    "The `nn.functional` package contains many useful loss functions and several other utilities."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "loss_fn = F.mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(1212.4377, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = loss_fn(model(inputs), targets)\n",
    "print(loss)"
   ]
  },
  {
   "source": [
    "Just as we no longer need to manually initialise the weights and biases, PyTorch uses an optimiser for manipulating the weights' and biases' gradients. This is `optim.SGD`, where the SGD stands for Stochastic Gradient Descent.\n",
    "\n",
    "The Learning Rate can also be specified so we can control the amount by which the parameters are modified."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "source": [
    "We generally define a utility function `fit` that trains the model for a given number of epochs."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to train the model\n",
    "def fit(num_epochs, model, loss_fn, opt):\n",
    "    \n",
    "    # Repeat for given number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Train with batches of data\n",
    "        for xb,yb in train_dl:\n",
    "            \n",
    "            # 1 - Generate the predictions\n",
    "            pred = model(xb)\n",
    "            \n",
    "            # 2 - Calculate the loss\n",
    "            loss = loss_fn(pred, yb)\n",
    "            \n",
    "            # 3 - Compute the gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # 4 - Update the parameters using gradients\n",
    "            opt.step()\n",
    "            \n",
    "            # 5 - Reset the gradients to zero\n",
    "            opt.zero_grad()\n",
    "        \n",
    "        # Print out the progress\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch [10/100], Loss: 69.4871\nEpoch [20/100], Loss: 71.2876\nEpoch [30/100], Loss: 48.2677\nEpoch [40/100], Loss: 59.9832\nEpoch [50/100], Loss: 22.2297\nEpoch [60/100], Loss: 38.4646\nEpoch [70/100], Loss: 40.3895\nEpoch [80/100], Loss: 91.7548\nEpoch [90/100], Loss: 53.3781\nEpoch [100/100], Loss: 19.6952\n"
     ]
    }
   ],
   "source": [
    "fit(100, model, loss_fn, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[43.5830, 27.3620],\n",
       "        [18.9134, 12.9657],\n",
       "        [11.7514,  5.9728],\n",
       "        [25.8749, 17.8681],\n",
       "        [23.1098, 12.9351],\n",
       "        [27.0564, 16.9283],\n",
       "        [24.0415, 16.0191],\n",
       "        [32.2245, 20.3997],\n",
       "        [16.6989,  7.1447],\n",
       "        [15.8383, 11.4294],\n",
       "        [34.3989, 25.8026],\n",
       "        [31.7742, 22.3330],\n",
       "        [28.9890, 17.1910],\n",
       "        [ 9.2068,  3.3394],\n",
       "        [16.9396,  9.6534]], grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[40., 20.],\n",
       "        [25., 15.],\n",
       "        [15., 10.],\n",
       "        [20., 15.],\n",
       "        [25., 20.],\n",
       "        [35., 25.],\n",
       "        [30., 25.],\n",
       "        [35., 25.],\n",
       "        [20., 12.],\n",
       "        [15., 10.],\n",
       "        [35., 10.],\n",
       "        [30., 17.],\n",
       "        [30., 26.],\n",
       "        [10., 16.],\n",
       "        [20., 14.]])"
      ]
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "source": [
    "# Compare with targets\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}